{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037ae04a",
   "metadata": {},
   "source": [
    "## TZ Survey Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc84b8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:05:01.550659Z",
     "start_time": "2023-09-08T20:05:01.520477Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4772068b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:05:02.229451Z",
     "start_time": "2023-09-08T20:05:01.529334Z"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e734721a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:05:04.594684Z",
     "start_time": "2023-09-08T20:05:04.587328Z"
    }
   },
   "outputs": [],
   "source": [
    "# The first step is to fetch all the data\n",
    "\n",
    "# determine the relative path of the folder holding the csv files\n",
    "folder_path = \"./TZA_2020_NPS-R5_v02_M_CSV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48b0529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-08T20:05:09.399124Z",
     "start_time": "2023-09-08T20:05:06.319319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the data\n",
    "data_dict = {}\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Create the variable name (removing the .csv extension)\n",
    "        var_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the CSV file into a DataFrame\n",
    "        data_dict[var_name] = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['ag_sec_4b', 'ag_filters', 'ag_sec_12b', 'npsy5.panel.key', 'hh_sec_q1', 'ag_sec_4a', 'cm_sec_f_id', 'ag_sec_12a', 'hh_sec_q2', 'hh_sec_e3', 'hh_sec_e2', 'ag_sec_5a', 'hh_sec_e1', 'hh_sec_i2', 'ag_sec_5b', 'cm_sec_g', 'hh_sec_a', 'hh_sec_v', 'cm_sec_f', 'cm_sec_d', 'lf_sec_08', 'hh_sec_c', 'hh_sec_b', 'cm_sec_e', 'hh_sec_ja1', 'cm_sec_a', 'hh_sec_f', 'hh_sec_p', 'hh_sec_g', 'hh_sec_o2', 'ag_sec_3b', 'cm_sec_b', 'hh_sec_r', 'hh_sec_s', 'hh_sec_d', 'hh_sec_o1', 'ag_sec_3a', 'cm_sec_c', 'hh_sec_u2', 'lf_sec_02', 'hh_sec_i', 'hh_sec_h', 'lf_sec_03', 'ag_sec_11', 'hh_sec_k', 'ag_sec_10', 'npsy5.child.anthro', 'lf_sec_04', 'consumption_real_y5', 'consumption_real_y4', 'hh_sec_n', 'lf_sec_05', 'ag_sec_01', 'lf_sec_07', 'hh_sec_l', 'hh_sec_m', 'lf_sec_06', 'ag_sec_02', 'hh_sec_j3', 'ag_sec_6b', 'lf_sec_04a', 'hh_sec_j1', 'ag_sec_6a', 'ag_sec_7a', 'hh_sec_j4', 'ag_sec_7b', 'cm_sec_d2'])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of all datasets:\n",
    "data_dict.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T20:06:31.774618Z",
     "start_time": "2023-09-08T20:06:31.766213Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the review of the data, it seems like the important datasets are:\n",
    "1. hh_sec_a {y5_hhid}\n",
    "2. hh_sec_b {y5_hhid, indidy5}\n",
    "3. hh_sec_e2 (occupation data) {y5_hhid, indidy5}\n",
    "3. hh_sec_e3 (firewood collection shows source of energy) {y5_hhid, indidy5}\n",
    "4. hh_seg_g (financial status) {y5_hhid, indidy5}\n",
    "5. hh_sec_j1 (food consumed) {y5_hhid, itemcode\n",
    "6. hh_sec_m (household assets) {y5_hhid, itemcode}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "hh_merged = data_dict['hh_sec_a'].\\\n",
    "    merge(data_dict['hh_sec_b'],\n",
    "                             how = 'left',\n",
    "                             on = 'y5_hhid',suffixes = ('_sx_a','_sx_b')).\\\n",
    "    merge(data_dict['hh_sec_e2'],\n",
    "           how = 'left',\n",
    "           left_on = ['y5_hhid', 'indidy5'],\n",
    "           right_on = ['y5_hhid', 'indidy5'], suffixes = ('_sx_ab','_sx_e2')).\\\n",
    "    merge(data_dict['hh_sec_e3'],\n",
    "           how = 'left',\n",
    "           left_on = ['y5_hhid', 'indidy5'],\n",
    "           right_on = ['y5_hhid', 'indidy5'],suffixes = ('_sx_abe2','_sx_e3')).\\\n",
    "    merge(data_dict['hh_sec_g'],\n",
    "           how = 'left',\n",
    "           left_on = ['y5_hhid', 'indidy5'],\n",
    "           right_on = ['y5_hhid', 'indidy5'],suffixes = ('_sx_abe2e3','_sx_g')).\\\n",
    "    merge(data_dict['hh_sec_j1'],\n",
    "           how = 'left',\n",
    "           left_on = 'y5_hhid',\n",
    "           right_on = 'y5_hhid',suffixes = ('_sx_abe2e3g','_sx_j1')).\\\n",
    "    merge(data_dict['hh_sec_m'],\n",
    "           how = 'left',\n",
    "           left_on = 'y5_hhid',\n",
    "           right_on = 'y5_hhid',suffixes = ('_sx_abe2e3gj1','_sx_m'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T21:33:18.018018Z",
     "start_time": "2023-09-08T21:32:28.509727Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#A bit of cleaning\n",
    "hh_merged_nona = hh_merged.dropna(axis=1, how='all')\n",
    "nan_columns = hh_merged_nona.columns[hh_merged_nona.eq('NaN').all()]\n",
    "hh_merged_nona = hh_merged_nona.drop(columns=nan_columns)\n",
    "\n",
    "confidential_columns = hh_merged_nona.columns[hh_merged_nona.eq('**CONFIDENTIAL**').all()]\n",
    "hh_merged_nona_nocon = hh_merged_nona.drop(columns=confidential_columns)\n",
    "\n",
    "\n",
    "columns_to_drop = []\n",
    "for column in hh_merged_nona_nocon.columns:\n",
    "    unique_values = hh_merged_nona_nocon[column].unique()\n",
    "    if len(unique_values) == 2 and 'NaN' in unique_values and '**CONFIDENTIAL**' in unique_values:\n",
    "        columns_to_drop.append(column)\n",
    "\n",
    "\n",
    "df_cleaned = hh_merged_nona_nocon.drop(columns=columns_to_drop)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-08T23:33:03.717339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_cleaned"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
